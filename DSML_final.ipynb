{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSML_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zPYp09exToZc"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9dPPoonS4FT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "549e3b35-62b7-4fa7-a4b1-20b99a69ecfe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCio_SO5TJE7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "578b4f68-5ad0-48f2-922a-022f44763b59"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# 匯入amazon unlocked mobile\n",
        "data = pd.read_csv('/content/drive/My Drive/Amazon_Unlocked_Mobile.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(413840, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Brand Name</th>\n",
              "      <th>Price</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Review Votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>5</td>\n",
              "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>5</td>\n",
              "      <td>Very pleased</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>It works good but it goes slow sometimes but i...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
              "      <td>Samsung</td>\n",
              "      <td>199.99</td>\n",
              "      <td>4</td>\n",
              "      <td>Great phone to replace my lost phone. The only...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Product Name  ... Review Votes\n",
              "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          1.0\n",
              "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  ...          0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdRq4W7mtgy3"
      },
      "source": [
        "import pandas as pd\n",
        "# 匯入yelp\n",
        "train_good = pd.read_table('/content/drive/My Drive/DSMLyelp/corpus.train.tsf.txt', header=None)\n",
        "train_bad = pd.read_table('/content/drive/My Drive/DSMLyelp/corpus.train.orig.txt', header=None)\n",
        "test_good = pd.read_table('/content/drive/My Drive/DSMLyelp/corpus.test.tsf.txt', header=None)\n",
        "test_bad = pd.read_table('/content/drive/My Drive/DSMLyelp/corpus.test.orig.txt', header=None)\n",
        "train_good.rename(columns={0:\"review\"}, inplace=True)\n",
        "train_bad.rename(columns={0:\"review\"}, inplace=True)\n",
        "test_good.rename(columns={0:\"review\"}, inplace=True)\n",
        "test_bad.rename(columns={0:\"review\"}, inplace=True)\n",
        "# 分別給yelp正向、負向資料label\n",
        "train_good['pos_neg'] = 1\n",
        "train_bad['pos_neg'] = 0\n",
        "test_good['pos_neg'] = 1\n",
        "test_bad['pos_neg'] = 0\n",
        "# yelp的訓練集和測試集\n",
        "train = pd.concat([train_good,train_bad], axis=0, ignore_index=True)\n",
        "test = pd.concat([test_good,test_bad], axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCb8XE6OAHBz"
      },
      "source": [
        "import re\n",
        "cList = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\", \n",
        "    \"ain 't\": \"am not\",\n",
        "  \"aren 't\": \"are not\",\n",
        "  \"can 't\": \"cannot\",\n",
        "  \"can 't've\": \"cannot have\",\n",
        "  \" 'cause\": \"because\",\n",
        "  \"could 've\": \"could have\",\n",
        "  \"couldn 't\": \"could not\",\n",
        "  \"couldn 't've\": \"could not have\",\n",
        "  \"didn 't\": \"did not\",\n",
        "  \"doesn 't\": \"does not\",\n",
        "  \"don 't\": \"do not\",\n",
        "  \"hadn 't\": \"had not\",\n",
        "  \"hadn 't've\": \"had not have\",\n",
        "  \"hasn 't\": \"has not\",\n",
        "  \"haven 't\": \"have not\",\n",
        "  \"he 'd\": \"he would\",\n",
        "  \"he 'd 've\": \"he would have\",\n",
        "  \"he 'll\": \"he will\",\n",
        "  \"he 'll 've\": \"he will have\",\n",
        "  \"he 's\": \"he is\",\n",
        "  \"how 'd\": \"how did\",\n",
        "  \"how 'd 'y\": \"how do you\",\n",
        "  \"how 'll\": \"how will\",\n",
        "  \"how 's\": \"how is\",\n",
        "  \"I 'd\": \"I would\",\n",
        "  \"I 'd've\": \"I would have\",\n",
        "  \"I 'll\": \"I will\",\n",
        "  \"I 'll've\": \"I will have\",\n",
        "  \"I 'm\": \"I am\",\n",
        "  \"I 've\": \"I have\",\n",
        "  \"isn 't\": \"is not\",\n",
        "  \"it 'd\": \"it had\",\n",
        "  \"it 'd 've\": \"it would have\",\n",
        "  \"it 'll\": \"it will\",\n",
        "  \"it 'll 've\": \"it will have\",\n",
        "  \"it 's\": \"it is\",\n",
        "  \"let 's\": \"let us\",\n",
        "  \"ma 'am\": \"madam\",\n",
        "  \"mayn 't\": \"may not\",\n",
        "  \"might 've\": \"might have\",\n",
        "  \"mightn 't\": \"might not\",\n",
        "  \"mightn 't 've\": \"might not have\",\n",
        "  \"must 've\": \"must have\",\n",
        "  \"mustn 't\": \"must not\",\n",
        "  \"mustn 't 've\": \"must not have\",\n",
        "  \"needn 't\": \"need not\",\n",
        "  \"needn 't 've\": \"need not have\",\n",
        "  \"o 'clock\": \"of the clock\",\n",
        "  \"oughtn 't\": \"ought not\",\n",
        "  \"oughtn 't 've\": \"ought not have\",\n",
        "  \"shan 't\": \"shall not\",\n",
        "  \"sha 'n 't\": \"shall not\",\n",
        "  \"shan 't 've\": \"shall not have\",\n",
        "  \"she 'd\": \"she would\",\n",
        "  \"she 'd 've\": \"she would have\",\n",
        "  \"she 'll\": \"she will\",\n",
        "  \"she 'll 've\": \"she will have\",\n",
        "  \"she 's\": \"she is\",\n",
        "  \"should 've\": \"should have\",\n",
        "  \"shouldn 't\": \"should not\",\n",
        "  \"shouldn 't 've\": \"should not have\",\n",
        "  \"so 've\": \"so have\",\n",
        "  \"so 's\": \"so is\",\n",
        "  \"that 'd\": \"that would\",\n",
        "  \"that 'd 've\": \"that would have\",\n",
        "  \"that 's\": \"that is\",\n",
        "  \"there 'd\": \"there had\",\n",
        "  \"there 'd 've\": \"there would have\",\n",
        "  \"there 's\": \"there is\",\n",
        "  \"they 'd\": \"they would\",\n",
        "  \"they 'd 've\": \"they would have\",\n",
        "  \"they 'll\": \"they will\",\n",
        "  \"they 'll 've\": \"they will have\",\n",
        "  \"they 're\": \"they are\",\n",
        "  \"they 've\": \"they have\",\n",
        "  \"to 've\": \"to have\",\n",
        "  \"wasn 't\": \"was not\",\n",
        "  \"we 'd\": \"we had\",\n",
        "  \"we 'd 've\": \"we would have\",\n",
        "  \"we 'll\": \"we will\",\n",
        "  \"we 'll 've\": \"we will have\",\n",
        "  \"we 're\": \"we are\",\n",
        "  \"we 've\": \"we have\",\n",
        "  \"weren 't\": \"were not\",\n",
        "  \"what 'll\": \"what will\",\n",
        "  \"what 'll 've\": \"what will have\",\n",
        "  \"what 're\": \"what are\",\n",
        "  \"what 's\": \"what is\",\n",
        "  \"what 've\": \"what have\",\n",
        "  \"when 's\": \"when is\",\n",
        "  \"when 've\": \"when have\",\n",
        "  \"where 'd\": \"where did\",\n",
        "  \"where 's\": \"where is\",\n",
        "  \"where 've\": \"where have\",\n",
        "  \"who 'll\": \"who will\",\n",
        "  \"who 'll 've\": \"who will have\",\n",
        "  \"who 's\": \"who is\",\n",
        "  \"who 've\": \"who have\",\n",
        "  \"why 's\": \"why is\",\n",
        "  \"why 've\": \"why have\",\n",
        "  \"will 've\": \"will have\",\n",
        "  \"won 't\": \"will not\",\n",
        "  \"won 't 've\": \"will not have\",\n",
        "  \"would 've\": \"would have\",\n",
        "  \"wouldn 't\": \"would not\",\n",
        "  \"wouldn 't 've\": \"would not have\",\n",
        "  \"y 'all\": \"you all\",\n",
        "  \"y 'alls\": \"you alls\",\n",
        "  \"y 'all 'd\": \"you all would\",\n",
        "  \"y 'all 'd've\": \"you all would have\",\n",
        "  \"y 'all 're\": \"you all are\",\n",
        "  \"y 'all 've\": \"you all have\",\n",
        "  \"you 'd\": \"you had\",\n",
        "  \"you 'd 've\": \"you would have\",\n",
        "  \"you 'll\": \"you you will\",\n",
        "  \"you 'll 've\": \"you you will have\",\n",
        "  \"you 're\": \"you are\",\n",
        "  \"you 've\": \"you have\"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I9UKyFbTKqy"
      },
      "source": [
        "# preprocessing\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def cleaning(text):\n",
        "  rm_tags = BeautifulSoup(text, 'lxml').text\n",
        "  rm_punctuations = rm_tags.translate(str.maketrans('', '', string.punctuation))\n",
        "  rm_numbers = re.sub('[0-9]+', '', rm_punctuations)\n",
        "  rm_spaces = rm_numbers.strip(\" \")\n",
        "  return rm_spaces\n",
        "\n",
        "# 消除空值\n",
        "def check_empty(text):\n",
        "  a=0\n",
        "  if bool(text.strip())== False:\n",
        "    a = 1\n",
        "  else:\n",
        "    a = 0\n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sho6NliVuU-a"
      },
      "source": [
        "# amazon 前處理\n",
        "data['Reviews']=data['Reviews'].(lambda x: str(x).lower())\n",
        "data['Reviews'] = data['Reviews'].apply(lambda x:expandContractions(str(x)))\n",
        "data['Reviews'] = data['Reviews'].apply(lambda x:cleaning(str(x)))\n",
        "data['null'] = data['Reviews'].apply(lambda x:check_empty(str(x)))\n",
        "data = data[data.null != 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vHQN7J2v_xX"
      },
      "source": [
        "# yelp前處理\n",
        "train['review'] = train['review'].apply(lambda x: x.lower())\n",
        "train['review'] = train['review'].apply(lambda x: expandContractions(str(x)))\n",
        "train['review'] = train['review'].apply(lambda x: cleaning(str(x)))\n",
        "train['null'] = train['text'].apply(lambda x:check_empty(str(x)))\n",
        "train = train[train.null != 1]\n",
        "\n",
        "test['review'] = test['review'].apply(lambda x: x.lower())\n",
        "test['review'] = test['review'].apply(lambda x: expandContractions(str(x)))\n",
        "test['review'] = test['review'].apply(lambda x: cleaning(str(x)))\n",
        "test['null'] = test['review'].apply(lambda x:check_empty(str(x)))\n",
        "test = test[test.null != 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUgknbigTPNr"
      },
      "source": [
        "# amazon的資料集大於3的要給label = 1 小於3的給label=0\n",
        "def f(row):\n",
        "    if row['Rating'] > 3:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val\n",
        "data['pos_neg'] = data.apply(f, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPun1RBITRLv"
      },
      "source": [
        "# 如果要用amaon的話\n",
        "new_data = pd.DataFrame()\n",
        "new_data['review'] = data['Reviews']\n",
        "new_data['pos_neg'] = data['pos_neg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_40A9wpswjot"
      },
      "source": [
        "# 如果要用yelp的話\n",
        "new_data = pd.DataFrame()\n",
        "new_data['review'] = train['Reviews']\n",
        "new_data['pos_neg'] = train['pos_neg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yborcBr188q-"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxcMIbijpcil",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "32393adb-577f-498a-a9be-602c4caeceba"
      },
      "source": [
        "# 只有amazon要這個動作\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(new_data['review'], new_data['pos_neg'], test_size=0.3, random_state = 0)\n",
        "print(\"X_train: \",X_train.shape)\n",
        "print(\"Y_train: \",Y_train.shape)\n",
        "print(\"X_test: \",X_test.shape)\n",
        "print(\"Y_test: \",Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:  (289585,)\n",
            "Y_train:  (289585,)\n",
            "X_test:  (124109,)\n",
            "Y_test:  (124109,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHrTAVX2pqH2"
      },
      "source": [
        "df_train = pd.concat([X_train,Y_train],axis =1)\n",
        "df_test = pd.concat([X_test,Y_test],axis =1)\n",
        "df_test = df_test.sample(n=1005, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHIJ5n3MrN1l"
      },
      "source": [
        "# Adjust Majority & minority class data ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kdIzcLmpMSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "a2087091-5d7a-4a8d-d1a6-e562468b5a35"
      },
      "source": [
        "df_majority = df_train[df_train.pos_neg==1]\n",
        "print('maj: ',df_majority)\n",
        "df_minority = df_train[df_train.pos_neg==0]\n",
        "print('min: ',df_minority)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maj:                                                     review  pos_neg\n",
            "363485  the product is exactly as advertised in the we...        1\n",
            "62298   i really like this phone a lot especially that...        1\n",
            "287554  good but watersweatdirt tends to go into thumb...        1\n",
            "141123              my likes it and it seems to work fine        1\n",
            "266407  i still have a full working e and can compare ...        1\n",
            "...                                                   ...      ...\n",
            "304242  im so glad that i found someone who still sell...        1\n",
            "359911                              everything is perfect        1\n",
            "358211                      excelent purchase as expected        1\n",
            "117994                                               good        1\n",
            "305816  i got mine refurbished from hd video depot abo...        1\n",
            "\n",
            "[199507 rows x 2 columns]\n",
            "min:                                                     review  pos_neg\n",
            "29      just not good the phone has great screen resol...        0\n",
            "360129  the front facing camera produces a failed erro...        0\n",
            "407770  nice little phone but keys are too small which...        0\n",
            "285873  color is more blue than purple in person with ...        0\n",
            "31291                   phone didnt last mor than a month        0\n",
            "...                                                   ...      ...\n",
            "220826  what a pity i think this phone is perfect in e...        0\n",
            "17096   extremely disappointing i ordered this phone b...        0\n",
            "170633  well the watch did what it was designed to do ...        0\n",
            "86322             that can i say its a bberry it was good        0\n",
            "152359  it was a nice phone i liked it except that the...        0\n",
            "\n",
            "[90078 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_idIOYRmpyMx"
      },
      "source": [
        "# ir 60\n",
        "df_majority60 = df_majority.sample(n=3000, random_state=0)\n",
        "df_minority60 = df_minority.sample(n=50, random_state=0)\n",
        "# ir 5\n",
        "df_majority5 = df_majority.sample(n=3000, random_state=0)\n",
        "df_minority5 = df_minority.sample(n=600, random_state=0)\n",
        "# ir 1.75\n",
        "df_majority175 = df_majority.sample(n=3000, random_state=0)\n",
        "df_minority175 = df_minority.sample(n=1714, random_state=0)\n",
        "# ir 1\n",
        "df_majority1 = df_majority.sample(n=3000, random_state=0)\n",
        "df_minority1 = df_minority.sample(n=3000, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCsAVSjtqG9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e46b2ac0-afc6-4165-a94a-e3eccddbe501"
      },
      "source": [
        "data_imbalanced = pd.concat([df_majority1,df_minority1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>pos_neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>224490</th>\n",
              "      <td>the phone overall is a great phone it takes gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119100</th>\n",
              "      <td>great phone very responsive literally takes da...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172184</th>\n",
              "      <td>none</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241847</th>\n",
              "      <td>for those of us using outdated phones these ca...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307534</th>\n",
              "      <td>i like the phone much better than the express ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390657</th>\n",
              "      <td>a month later the microphone for calls was rui...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177829</th>\n",
              "      <td>this phone looks and feel good the size is jus...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194160</th>\n",
              "      <td>cannot get update from any carriers not att no...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40911</th>\n",
              "      <td>how do i give it zero stars phone was still un...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336653</th>\n",
              "      <td>i bought this phone one a year a ago and now i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review  pos_neg\n",
              "224490  the phone overall is a great phone it takes gr...        1\n",
              "119100  great phone very responsive literally takes da...        1\n",
              "172184                                               none        1\n",
              "241847  for those of us using outdated phones these ca...        1\n",
              "307534  i like the phone much better than the express ...        1\n",
              "...                                                   ...      ...\n",
              "390657  a month later the microphone for calls was rui...        0\n",
              "177829  this phone looks and feel good the size is jus...        0\n",
              "194160  cannot get update from any carriers not att no...        0\n",
              "40911   how do i give it zero stars phone was still un...        0\n",
              "336653  i bought this phone one a year a ago and now i...        0\n",
              "\n",
              "[6000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-npNsdYwwyu"
      },
      "source": [
        "# TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePqdO-HywyII",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "41608e91-1c98-4e30-ae04-445551e9fb56"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "X_train = data_imbalanced['review']\n",
        "Y_train = data_imbalanced['pos_neg']\n",
        "X_test = df_test['review']\n",
        "Y_test = df_test['pos_neg']\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(X_train)\n",
        "X_train = tfidf.transform(X_train)\n",
        "X_test = tfidf.transform(X_test)\n",
        "print(\"X_train: \",X_train.shape)\n",
        "print(\"Y_train: \",Y_train.shape)\n",
        "print(\"X_test: \",X_test.shape)\n",
        "print(\"Y_test: \",Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train:  (6000, 11809)\n",
            "Y_train:  (6000,)\n",
            "X_test:  (1005, 11809)\n",
            "Y_test:  (1005,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QT6PU89TVqB"
      },
      "source": [
        "# SBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzsjW41XTW6x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "outputId": "c6cc1037-3646-4505-f9b0-f45996232791"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2020.4.5.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.8.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp36-none-any.whl size=74031 sha256=625356e89b427144022b80937699ae24d207a301daf5006d6f7137aae175062e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a8ace53f86f978fa0a7cb06692175b24504d1ec4b734c223f25b7e8c44117f2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUgxNHMdTbjN"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "np.set_printoptions(threshold=100)\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout\n",
        "\n",
        "\n",
        "\n",
        "# Load Sentence model (based on BERT) from URL\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBvKsBV2aHTg"
      },
      "source": [
        "# Embed a list of sentences\n",
        "def sentence_embeddings(sentences):\n",
        "  sentence_embeddings = model.encode(sentences)\n",
        "  return sentence_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MJ6pnq-TeX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bbce0d7-0cd3-41f9-8cd0-0cbeeba687cd"
      },
      "source": [
        "data_imbalanced['sbert'] = sentence_embeddings(np.array(data_imbalanced['review']))\n",
        "X_train = data_imbalanced['sbert']\n",
        "Y_train = data_imbalanced['pos_neg']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 590/590 [00:32<00:00, 18.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7CcdcxPKIHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28fb6ce2-de74-47d7-b8d4-3af55881c831"
      },
      "source": [
        "df_test['sbert'] = sentence_embeddings(np.array(df_test['review']))\n",
        "X_test = df_test['sbert']\n",
        "Y_test = df_test['pos_neg']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 126/126 [00:06<00:00, 18.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPYp09exToZc"
      },
      "source": [
        "# SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIYMtcHiT8jw"
      },
      "source": [
        "# representation用SBERT的話才需要tolist()\n",
        "X_train = X_train.values.tolist()\n",
        "X_test = X_test.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQQnC85UAQl"
      },
      "source": [
        "# smote to 1:1\n",
        "from imblearn.over_sampling import SMOTE\n",
        "over_samples = SMOTE(k_neighbors=5, random_state = 0) \n",
        "over_samples_X,over_samples_y = over_samples.fit_sample(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UneRLvYFUBLO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "510b08d4-a1af-4b09-8644-1fa1bc57d9a7"
      },
      "source": [
        "# 重抽样前的类别比例\n",
        "print(Y_train.value_counts()/len(Y_train))\n",
        "# 重抽样后的类别比例\n",
        "print(pd.Series(over_samples_y).value_counts()/len(over_samples_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    0.5\n",
            "0    0.5\n",
            "Name: pos_neg, dtype: float64\n",
            "1    0.5\n",
            "0    0.5\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOn9WVfGIunM"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R3mCL2UvtRo"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "clf = LinearSVC()\n",
        "clf.fit(over_samples_X,over_samples_y)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"SVM f score: \",f1_score(Y_test, y_pred, average='macro'))\n",
        "print(\"SVM gm: \",geometric_mean_score(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCCiXip-vwar"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "clf.fit(over_samples_X, over_samples_y)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"DT f score: \",f1_score(Y_test, y_pred, average='macro'))\n",
        "print(\"DT gm: \",geometric_mean_score(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fV8I6ohvxpI"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0, C=1.0).fit(over_samples_X, over_samples_y)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"LR f score: \",f1_score(Y_test, y_pred, average='macro'))\n",
        "print(\"LR gm: \",geometric_mean_score(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTGzXMSgCNz5"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(over_samples_X, over_samples_y)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"NB f score: \",f1_score(Y_test, y_pred, average='macro'))\n",
        "print(\"BN gm: \",geometric_mean_score(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsLho5NSvsdH"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "clf = GaussianNB()\n",
        "clf.fit(over_samples_X, over_samples_y)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"NB f score: \",f1_score(Y_test, y_pred, average='macro'))\n",
        "print(\"BN gm: \",geometric_mean_score(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}